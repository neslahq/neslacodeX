model:
  n_embd: 768
  n_head: 12
  n_layers: 12
  block_size: 1024
  vocab_size: 50304
  optimizer:
    name: AdamW
    learning_rate: 6e-4
    weight_decay: 0.1
    max_lr: 6e-4
    min_lr: $max_lr * 0.1
    warmup_steps: 715

data:
  path: ~/neslacodeX/src/data/input.txt

train:
  epochs: 1
  max_steps: 50
  