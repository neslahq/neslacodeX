model:
  n_embd: 768
  n_head: 12
  n_layers: 12
  n_exp: 2 
  top_k: 2
  capacity_factor: 2
  use_router_noise: true
  bias: true
  use_moe: true
  block_size: 1024
  vocab_size: 50304
  dropout: 0.1
  optimizer:
    name: AdamW
    learning_rate: 6e-4
    weight_decay: 0.1
    max_lr: 6e-4
    min_lr: 6e-5
    warmup_steps: 10

data:
  path: ~/neslacodeX/src/data/input.txt

train:
  epochs: 1
  max_steps: 50
  total_batch_size: 524288  